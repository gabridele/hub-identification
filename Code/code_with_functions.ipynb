{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments needed:\n",
    "\n",
    "* get_degree: folder_path (input of adjacency matrices.csv), output_path\n",
    "* get_hub: output_folder (input of csv files with indexes and degree), hub_folder (output)\n",
    "* unique_hub: hub_folder (input of folder with many .csv files with selected hubs and their degree), unique_hub_file_path (output where to store single .csv file)\n",
    "* extract_coordinates: t1_path, atlas_path, adjacency_path (any adjacency matrix among subjects), output_file (where to store adjacency with coordinates)\n",
    "* hub_coord: hub_path, coordinates_path, output_merged (where to store output)\n",
    "* node_file: hub_coord_path, output_path (path to store ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get degree of nodes in all adjacency matrices in a folder. Output gets stored in csv file\n",
    "\n",
    "def get_degree(folder_path, degree_file_path):\n",
    "    # List all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    # Iterate through each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(folder_path, csv_file)\n",
    "\n",
    "        # Read CSV file into a numpy array\n",
    "        mydata = np.genfromtxt(file_path, delimiter=',')\n",
    "\n",
    "        # Get degree from graph\n",
    "        G = nx.Graph(mydata)\n",
    "        w = G.degree()\n",
    "\n",
    "        # Post-processing - cleanup\n",
    "        output_data = np.array(list(w))\n",
    "\n",
    "        # Remove decimals\n",
    "        output_data[:, 0] = np.round(output_data[:, 0])\n",
    "\n",
    "        # Format the second column to keep only the first 3 digits\n",
    "        output_data[:, 1] = output_data[:, 1]\n",
    "\n",
    "        # Save results to a new CSV file in cleaned output folder\n",
    "        degree_file_path = os.path.join(degree_file_path, f'cleaned_{csv_file}')\n",
    "        np.savetxt(degree_file_path, output_data, delimiter=',', fmt='%d', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From csv file with node degrees, pick only the ones that are hubs (those > SD)\n",
    "\n",
    "def get_hub(degree_file_path, hub_folder):\n",
    "    # List all CSV files\n",
    "    degree_files = [f for f in os.listdir(degree_file_path) if f.endswith('.csv')]\n",
    "\n",
    "    for degree_file in degree_files:\n",
    "        degree_file_path = os.path.join(degree_file_path, degree_file)\n",
    "\n",
    "        # Load data from CSV file\n",
    "        degree_data = pd.read_csv(degree_file_path, header=None)\n",
    "\n",
    "        # Calculate mean and standard deviation of the second column\n",
    "        mean_value = degree_data.iloc[:, 1].mean()\n",
    "        std_dev = degree_data.iloc[:, 1].std()\n",
    "\n",
    "        # Create a new dataframe with values greater than 1 SD from the mean\n",
    "        hub_data = degree_data[degree_data.iloc[:, 1] > (mean_value + std_dev)]\n",
    "\n",
    "        # Save to a CSV file in the hub folder\n",
    "        hub_file_path = os.path.join(hub_folder, degree_file)\n",
    "        hub_data.to_csv(hub_file_path, index=False, header=False)\n",
    "\n",
    "    print(\"Hub files created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get single hub file\n",
    "\n",
    "def unique_hub(hub_folder, unique_hub_file_path):\n",
    "    # List all CSV files hub folder\n",
    "    hub_files = [f for f in os.listdir(hub_folder) if f.endswith('.csv')]\n",
    "\n",
    "    # Dictionary to store data for each index\n",
    "    index_data = {}\n",
    "\n",
    "    # Track how many files each index is present in\n",
    "    index_presence_counter = {}\n",
    "\n",
    "    # Iterate through each hub file\n",
    "    for hub_file in hub_files:\n",
    "        hub_file_path = os.path.join(hub_folder, hub_file)\n",
    "\n",
    "        # Load the data from hub CSV file\n",
    "        hub_data = pd.read_csv(hub_file_path, header=None)\n",
    "\n",
    "        # Iterate through each row in the current file\n",
    "        for _, row in hub_data.iterrows():\n",
    "            index = row.iloc[0]\n",
    "            value = float(row.iloc[1])\n",
    "\n",
    "            # Update the data dictionary for the current index\n",
    "            if index in index_data:\n",
    "                index_data[index].append(value)\n",
    "                index_presence_counter[index] += 1\n",
    "            else:\n",
    "                index_data[index] = [value]\n",
    "                index_presence_counter[index] = 1\n",
    "\n",
    "    # Filter indices that are present in more than n% of files\n",
    "    common_indices = [index for index, count in index_presence_counter.items() if count >= 0.95 * len(hub_files)]\n",
    "\n",
    "    # Create a df with the average values for each common index\n",
    "    average_data = {'Index': [], 'AverageDegreeVal': []}\n",
    "\n",
    "    for index in common_indices:\n",
    "        # Get average value for the current index\n",
    "        average_value = np.mean(index_data[index])\n",
    "        average_data['Index'].append(index)\n",
    "        average_data['AverageDegreeVal'].append(average_value)\n",
    "\n",
    "    # Convert dictionary to a DataFrame\n",
    "    unique_hub_data = pd.DataFrame(average_data)\n",
    "    unique_hub_data = unique_hub_data.round({'AverageDegreeVal': 1})\n",
    "\n",
    "    # Save to CSV file\n",
    "    unique_hub_data.to_csv(unique_hub_file_path, index=False)\n",
    "\n",
    "    print(\"Unique hub file created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use an adjacency matrix to extract coordinates from t1 and atlas\n",
    "\n",
    "def extract_coordinates(t1_path, atlas_path, adjacency_path, output_file):\n",
    "    # Load the brain image and brain atlas\n",
    "    t1_image = nib.load(t1_path)\n",
    "    brain_atlas = nib.load(atlas_path)\n",
    "\n",
    "    # Load the adjacency matrix\n",
    "    adjacency_matrix = pd.read_csv(adjacency_path, header=None)\n",
    "\n",
    "    # Extract the data\n",
    "    atlas_data = brain_atlas.get_fdata()\n",
    "\n",
    "    # Extract the unique labels from the brain atlas\n",
    "    unique_labels = np.unique(brain_atlas.get_fdata())\n",
    "    # Drop the 0 here\n",
    "\n",
    "    # Convert voxel coordinates to world coordinates\n",
    "    def voxel_to_world_coordinates(voxel_coords, affine_matrix):\n",
    "        world_coords = [affine_matrix[:3, :3].dot(voxel) + affine_matrix[:3, 3] for voxel in voxel_coords]\n",
    "        return np.array(world_coords)\n",
    "\n",
    "    # Iterate through unique labels in the atlas\n",
    "    node_coordinates = []\n",
    "    for label in unique_labels:\n",
    "        # Find voxel indices with the current label\n",
    "        indices = np.where(brain_atlas.get_fdata() == label)\n",
    "        \n",
    "        # Use the centroid of the voxels as node coordinates\n",
    "        centroid = np.mean(np.array(indices), axis=1)\n",
    "        node_coordinates.append(centroid)\n",
    "\n",
    "    # Convert voxel coordinates to world coordinates\n",
    "    world_coordinates = voxel_to_world_coordinates(node_coordinates, t1_image.affine)\n",
    "\n",
    "    # Convert into dataframe\n",
    "    w_c2 = pd.DataFrame(world_coordinates)\n",
    "\n",
    "    # Drop first row of coordinates as it represents the non-brain\n",
    "    w_c2 = w_c2.drop([0])\n",
    "\n",
    "    # Adjust indexes to make them align\n",
    "    adjacency_matrix.reset_index(drop=True, inplace=True)\n",
    "    w_c2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Concatenate the two dataframes\n",
    "    adjacency_matrix_with_coordinates = pd.concat([adjacency_matrix, w_c2], axis=1)\n",
    "\n",
    "    adjacency_matrix_with_coordinates.to_csv(output_file)\n",
    "    print(\"Adjacency with coordinates created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get hub coordinates\n",
    "\n",
    "def hub_coord(hub_path, coordinates_path, output_merged):\n",
    "    # Load unique hub file\n",
    "    unique_hub_data = pd.read_csv(hub_path, index_col=0)\n",
    "\n",
    "    # Load the CSV file containing X, Y, Z coordinates\n",
    "    coordinates_data = pd.read_csv(coordinates_path, index_col=0)\n",
    "\n",
    "    # Get the last three columns containing the xyz coordinates\n",
    "    new_coord = coordinates_data.iloc[:,-3:]\n",
    "    new_coord.columns = [\"X\", \"Y\", \"Z\"]\n",
    "    new_coord.index.name = 'Index'\n",
    "\n",
    "    new_coord.index += 1\n",
    "\n",
    "    # Merge dataframes based on the common 'Index' column\n",
    "    merged_data = unique_hub_data.merge(new_coord, left_on='Index', right_on='Index')\n",
    "    merged_data = merged_data.sort_values(by=['Index'])\n",
    "\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_data.to_csv(output_merged, index=True)\n",
    "\n",
    "    print(\"Hub with coordinates created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get .node file for rendering onto brain surface\n",
    "\n",
    "def node_file(hub_coord_path, output_path):\n",
    "    # Read hub with coordinates file\n",
    "    hub_coord = pd.read_csv(hub_coord_path, delimiter=',', skipinitialspace=True)\n",
    "\n",
    "    # Extract x y z columns\n",
    "    coordinates = hub_coord[['X', 'Y', 'Z']]\n",
    "\n",
    "    # Prepare data frame\n",
    "    node_data = pd.DataFrame({\n",
    "        'X': coordinates['X'],\n",
    "        'Y': coordinates['Y'],\n",
    "        'Z': coordinates['Z'],\n",
    "    })\n",
    "\n",
    "    # Concatenate columns with space and 1s\n",
    "    # The coordinates for the center of each sphere are defined by the first three columns, \n",
    "    # the fourth column defines the intensity and the fifth column defines the radius.\n",
    "    # Here we set 1 and 1 as default for 4th and 5th column\n",
    "    node_data['Coordinates'] = node_data['X'].astype(str) + \" \" + node_data['Y'].astype(str) + \" \" + node_data['Z'].astype(str) + \" \" + \"1\" + \" \" + \"1\"\n",
    "    node_data = node_data['Coordinates']\n",
    "\n",
    "    # Save .node file\n",
    "    node_data.to_csv(output_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "    print(\".node file created\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop_weizmann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
