{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From connectome matrices get the degree of each node in every file. Store node index and degree in a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degrees computed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# Paths\n",
    "folder_path = '../gabriele_sc_connectomes'\n",
    "cleaned_output_folder = '../code&output/1_outputs_cleaned'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Iterate through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "\n",
    "    # Read CSV file into a numpy array\n",
    "    mydata = np.genfromtxt(file_path, delimiter=',')\n",
    "\n",
    "    # Get degree from graph\n",
    "    G = nx.Graph(mydata)\n",
    "    w = G.degree()\n",
    "\n",
    "    # Post-processing - cleanup\n",
    "    output_data = np.array(list(w))\n",
    "\n",
    "    # Remove decimals\n",
    "    output_data[:, 0] = np.round(output_data[:, 0])\n",
    "\n",
    "    # Format the second column to keep only the first 3 digits\n",
    "    output_data[:, 1] = output_data[:, 1]\n",
    "\n",
    "    # Save results to a new CSV file in cleaned output folder\n",
    "    cleaned_output_path = os.path.join(cleaned_output_folder, f'cleaned_{csv_file}')\n",
    "    np.savetxt(cleaned_output_path, output_data, delimiter=',', fmt='%d', comments='')\n",
    "\n",
    "print(\"Degrees computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get those nodes that are eligible to be hubs (with degree greater than 1 SD from mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hub files created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder paths\n",
    "cleaned_folder = '../code&output/1_outputs_cleaned'\n",
    "hub_folder = '../code&output/2_hub'\n",
    "\n",
    "# List all CSV files\n",
    "cleaned_files = [f for f in os.listdir(cleaned_folder) if f.endswith('.csv')]\n",
    "\n",
    "for cleaned_file in cleaned_files:\n",
    "    cleaned_file_path = os.path.join(cleaned_folder, cleaned_file)\n",
    "\n",
    "    # Load data from CSV file\n",
    "    cleaned_data = pd.read_csv(cleaned_file_path, header=None)\n",
    "\n",
    "    # Calculate mean and standard deviation of the second column\n",
    "    mean_value = cleaned_data.iloc[:, 1].mean()\n",
    "    std_dev = cleaned_data.iloc[:, 1].std()\n",
    "\n",
    "    # Create a new dataframe with values greater than 1 SD from the mean\n",
    "    hub_data = cleaned_data[cleaned_data.iloc[:, 1] > (mean_value + std_dev)]\n",
    "\n",
    "    # Save to a CSV file in the hub folder\n",
    "    hub_file_path = os.path.join(hub_folder, cleaned_file)\n",
    "    hub_data.to_csv(hub_file_path, index=False, header=False)\n",
    "\n",
    "print(\"Hub files created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get unique hub file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hub file created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Folder paths\n",
    "hub_folder = '../code&output/2_hub'\n",
    "unique_hub_file_path = '../code&output/3_unique_hub/unique_hub.csv'\n",
    "\n",
    "# List all CSV files hub folder\n",
    "hub_files = [f for f in os.listdir(hub_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Dictionary to store data for each index\n",
    "index_data = {}\n",
    "\n",
    "# Track how many files each index is present in\n",
    "index_presence_counter = {}\n",
    "\n",
    "# Iterate through each hub file\n",
    "for hub_file in hub_files:\n",
    "    hub_file_path = os.path.join(hub_folder, hub_file)\n",
    "\n",
    "    # Load the data from hub CSV file\n",
    "    hub_data = pd.read_csv(hub_file_path, header=None)\n",
    "\n",
    "    # Iterate through each row in the current file\n",
    "    for _, row in hub_data.iterrows():\n",
    "        index = row.iloc[0]\n",
    "        value = float(row.iloc[1])\n",
    "\n",
    "        # Update the data dictionary for the current index\n",
    "        if index in index_data:\n",
    "            index_data[index].append(value)\n",
    "            index_presence_counter[index] += 1\n",
    "        else:\n",
    "            index_data[index] = [value]\n",
    "            index_presence_counter[index] = 1\n",
    "\n",
    "# Filter indices that are present in more than n% of files\n",
    "common_indices = [index for index, count in index_presence_counter.items() if count >= 0.95 * len(hub_files)]\n",
    "\n",
    "# Create a df with the average values for each common index\n",
    "average_data = {'Index': [], 'AverageDegreeVal': []}\n",
    "\n",
    "for index in common_indices:\n",
    "    # Get average value for the current index\n",
    "    average_value = np.mean(index_data[index])\n",
    "    average_data['Index'].append(index)\n",
    "    average_data['AverageDegreeVal'].append(average_value)\n",
    "\n",
    "# Convert dictionary to a DataFrame\n",
    "unique_hub_data = pd.DataFrame(average_data)\n",
    "unique_hub_data = unique_hub_data.round({'AverageDegreeVal': 1})\n",
    "\n",
    "# Save to CSV file\n",
    "unique_hub_data.to_csv(unique_hub_file_path, index=False)\n",
    "\n",
    "print(\"Unique hub file created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use any adjacency matrix to extract the coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the brain image and brain atlas\n",
    "t1_image = nib.load('../templates_atlas/MNI152_T1_2mm.nii')\n",
    "brain_atlas = nib.load('../templates_atlas/yeo_tian_2mm.nii')\n",
    "\n",
    "# Load the adjacency matrix\n",
    "adjacency_matrix = pd.read_csv('../gabriele_sc_connectomes/connectome_sub-100206.csv', header=None)\n",
    "\n",
    "# Extract the data\n",
    "atlas_data = brain_atlas.get_fdata()\n",
    "\n",
    "# Extract the unique labels from the brain atlas\n",
    "unique_labels = np.unique(brain_atlas.get_fdata())\n",
    "# Drop the 0 here\n",
    "\n",
    "# Convert voxel coordinates to world coordinates\n",
    "def voxel_to_world_coordinates(voxel_coords, affine_matrix):\n",
    "    world_coords = [affine_matrix[:3, :3].dot(voxel) + affine_matrix[:3, 3] for voxel in voxel_coords]\n",
    "    return np.array(world_coords)\n",
    "\n",
    "# Iterate through unique labels in the atlas\n",
    "node_coordinates = []\n",
    "for label in unique_labels:\n",
    "    # Find voxel indices with the current label\n",
    "    indices = np.where(brain_atlas.get_fdata() == label)\n",
    "    \n",
    "    # Use the centroid of the voxels as node coordinates\n",
    "    centroid = np.mean(np.array(indices), axis=1)\n",
    "    node_coordinates.append(centroid)\n",
    "\n",
    "# Convert voxel coordinates to world coordinates\n",
    "world_coordinates = voxel_to_world_coordinates(node_coordinates, t1_image.affine)\n",
    "\n",
    "# Convert into dataframe\n",
    "w_c2 = pd.DataFrame(world_coordinates)\n",
    "\n",
    "# Drop first row of coordinates as it represents the non-brain\n",
    "w_c2 = w_c2.drop([0])\n",
    "\n",
    "# Adjust indexes to make them align\n",
    "adjacency_matrix.reset_index(drop=True, inplace=True)\n",
    "w_c2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "adjacency_matrix_with_coordinates = pd.concat([adjacency_matrix, w_c2], axis=1)\n",
    "\n",
    "adjacency_matrix_with_coordinates.to_csv('../code&output/4_adj_coord/adjacency_matrix_with_coordinates.csv')\n",
    "print(\"Adjacency with coordinates created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Get hub coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hub with coordinates created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load unique hub file\n",
    "unique_hub_data = pd.read_csv('../code&output/3_unique_hub/unique_hub.csv', index_col=0)\n",
    "\n",
    "# Load the CSV file containing X, Y, Z coordinates\n",
    "coordinates_data = pd.read_csv('../code&output/4_adj_coord/adjacency_matrix_with_coordinates.csv', index_col=0)\n",
    "\n",
    "# Get the last three columns containing the xyz coordinates\n",
    "new_coord = coordinates_data.iloc[:,-3:]\n",
    "new_coord.columns = [\"X\", \"Y\", \"Z\"]\n",
    "new_coord.index.name = 'Index'\n",
    "\n",
    "new_coord.index += 1\n",
    "\n",
    "# Merge dataframes based on the common 'Index' column\n",
    "merged_data = unique_hub_data.merge(new_coord, left_on='Index', right_on='Index')\n",
    "merged_data = merged_data.sort_values(by=['Index'])\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_data.to_csv('../code&output/5_hub_coord/hub_with_coordinates.csv', index=True)\n",
    "\n",
    "print(\"Hub with coordinates created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Get .node file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".node file created\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load hub with coordinates file\n",
    "hub_coord_path = '../code&output/5_hub_coord/hub_with_coordinates.csv'\n",
    "hub_coord = pd.read_csv(hub_coord_path, delimiter=',', skipinitialspace=True)\n",
    "\n",
    "# Extract x y z columns\n",
    "coordinates = hub_coord[['X', 'Y', 'Z']]\n",
    "\n",
    "# Prepare data frame\n",
    "node_data = pd.DataFrame({\n",
    "    'X': coordinates['X'],\n",
    "    'Y': coordinates['Y'],\n",
    "    'Z': coordinates['Z'],\n",
    "})\n",
    "\n",
    "# Concatenate columns with space and 1s\n",
    "# The coordinates for the center of each sphere are defined by the first three columns, \n",
    "# the fourth column defines the intensity and the fifth column defines the radius.\n",
    "# Here we set 1 and 1 as default for 4th and 5th column\n",
    "node_data['Coordinates'] = node_data['X'].astype(str) + \" \" + node_data['Y'].astype(str) + \" \" + node_data['Z'].astype(str) + \" \" + \"1\" + \" \" + \"1\"\n",
    "node_data = node_data['Coordinates']\n",
    "\n",
    "# Save .node file\n",
    "output_path = '../code&output/6_node_file/file.node'\n",
    "node_data.to_csv(output_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "print(\".node file created\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
